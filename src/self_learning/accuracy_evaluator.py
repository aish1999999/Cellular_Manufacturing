"""
Accuracy Evaluation Module

This module evaluates the accuracy and quality of answers compared to
expected answers and source material.
"""

import json
from typing import Dict, Any, List, Optional
from openai import OpenAI
import numpy as np


class AccuracyEvaluator:
    """
    Evaluates answer accuracy using multiple metrics.

    Features:
    - Semantic similarity evaluation
    - Factual accuracy checking
    - Completeness assessment
    - Citation quality evaluation
    - LLM-based quality scoring
    """

    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        """
        Initialize the accuracy evaluator.

        Args:
            api_key: OpenAI API key
            model: LLM model to use for evaluation
        """
        self.client = OpenAI(api_key=api_key)
        self.model = model

    def evaluate_single_answer(
        self,
        question: str,
        generated_answer: str,
        expected_answer: str = None,
        source_text: str = None,
        sources: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate a single answer against expected answer and sources.

        Args:
            question: The question asked
            generated_answer: The answer generated by the system
            expected_answer: The expected/reference answer (optional)
            source_text: Original source text (optional)
            sources: Source documents used (optional)

        Returns:
            Dictionary with evaluation scores and feedback
        """
        evaluation = {
            'question': question,
            'generated_answer': generated_answer,
            'expected_answer': expected_answer
        }

        # LLM-based evaluation
        llm_eval = self._llm_based_evaluation(
            question=question,
            generated_answer=generated_answer,
            expected_answer=expected_answer,
            source_text=source_text
        )

        evaluation.update(llm_eval)

        # Citation quality check (if sources provided)
        if sources:
            citation_score = self._evaluate_citations(generated_answer, sources)
            evaluation['citation_quality'] = citation_score

        # Calculate composite score
        evaluation['composite_score'] = self._calculate_composite_score(evaluation)

        return evaluation

    def _llm_based_evaluation(
        self,
        question: str,
        generated_answer: str,
        expected_answer: str = None,
        source_text: str = None
    ) -> Dict[str, Any]:
        """
        Use LLM to evaluate answer quality.

        Returns scores for:
        - Accuracy (0-10)
        - Completeness (0-10)
        - Relevance (0-10)
        - Clarity (0-10)
        """
        prompt = f"""Evaluate the quality of the generated answer to the question.

Question: {question}

Generated Answer: {generated_answer}
"""

        if expected_answer:
            prompt += f"\nExpected Answer: {expected_answer}"

        if source_text:
            prompt += f"\n\nSource Material:\n{source_text[:1000]}..."

        prompt += """

Evaluate the generated answer on these criteria (score 0-10 for each):

1. **Accuracy**: Is the answer factually correct?
2. **Completeness**: Does it fully answer the question?
3. **Relevance**: Is the answer relevant to the question?
4. **Clarity**: Is the answer clear and well-structured?

Also provide:
- **Strengths**: What the answer does well
- **Weaknesses**: What could be improved
- **Missing Information**: Important information not included

Return as JSON:
{
    "accuracy_score": 0-10,
    "completeness_score": 0-10,
    "relevance_score": 0-10,
    "clarity_score": 0-10,
    "strengths": ["strength1", "strength2"],
    "weaknesses": ["weakness1", "weakness2"],
    "missing_information": ["missing1", "missing2"],
    "overall_feedback": "Brief summary of evaluation"
}
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert evaluator assessing the quality of question-answering systems. Provide detailed, objective evaluations."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            result = json.loads(content)

            return result

        except Exception as e:
            print(f"Error in LLM evaluation: {e}")
            return {
                'accuracy_score': 0,
                'completeness_score': 0,
                'relevance_score': 0,
                'clarity_score': 0,
                'error': str(e)
            }

    def _evaluate_citations(
        self,
        answer: str,
        sources: List[Dict[str, Any]]
    ) -> float:
        """
        Evaluate citation quality.

        Args:
            answer: The generated answer
            sources: Source documents used

        Returns:
            Citation quality score (0-1)
        """
        if not sources:
            return 0.0

        # Check for page citations in answer
        import re
        citations = re.findall(r'\[Page \d+\]', answer)

        if not citations:
            return 0.2  # Answer has sources but no citations

        # Check if citations match source pages
        cited_pages = set(int(re.search(r'\d+', c).group()) for c in citations)
        source_pages = set(s.get('page', 0) for s in sources)

        if not source_pages:
            return 0.5

        # Calculate overlap
        overlap = len(cited_pages & source_pages) / len(cited_pages)

        # Penalize over-citation or under-citation
        citation_ratio = len(citations) / len(answer.split('.'))
        if citation_ratio > 0.5:  # Too many citations
            overlap *= 0.8
        elif len(citations) < len(sources) / 2:  # Too few citations
            overlap *= 0.9

        return overlap

    def _calculate_composite_score(self, evaluation: Dict[str, Any]) -> float:
        """
        Calculate composite score from individual metrics.

        Args:
            evaluation: Dictionary with evaluation scores

        Returns:
            Composite score (0-10)
        """
        weights = {
            'accuracy_score': 0.35,
            'completeness_score': 0.25,
            'relevance_score': 0.20,
            'clarity_score': 0.15,
            'citation_quality': 0.05
        }

        composite = 0.0
        total_weight = 0.0

        for metric, weight in weights.items():
            if metric in evaluation:
                value = evaluation[metric]
                # Normalize citation_quality to 0-10 scale
                if metric == 'citation_quality':
                    value = value * 10
                composite += value * weight
                total_weight += weight

        if total_weight > 0:
            composite = composite / total_weight

        return round(composite, 2)

    def evaluate_batch(
        self,
        qa_results: List[Dict[str, Any]],
        expected_answers: Dict[str, str] = None
    ) -> Dict[str, Any]:
        """
        Evaluate multiple Q&A pairs.

        Args:
            qa_results: List of Q&A result dictionaries
            expected_answers: Optional dict mapping questions to expected answers

        Returns:
            Dictionary with batch evaluation results and statistics
        """
        evaluations = []
        total = len(qa_results)

        print(f"\nEvaluating {total} Q&A pairs...")

        for i, qa in enumerate(qa_results, 1):
            question = qa.get('question', '')
            answer = qa.get('answer', '')
            sources = qa.get('sources', [])

            # Get expected answer if available
            expected = None
            if expected_answers and question in expected_answers:
                expected = expected_answers[question]
            elif 'question_metadata' in qa:
                expected = qa['question_metadata'].get('expected_answer')

            print(f"  [{i}/{total}] Evaluating...", end=" ")

            eval_result = self.evaluate_single_answer(
                question=question,
                generated_answer=answer,
                expected_answer=expected,
                sources=sources
            )

            evaluations.append(eval_result)
            print(f"Score: {eval_result['composite_score']:.1f}/10")

        # Calculate aggregate statistics
        scores = {
            'accuracy': [e.get('accuracy_score', 0) for e in evaluations],
            'completeness': [e.get('completeness_score', 0) for e in evaluations],
            'relevance': [e.get('relevance_score', 0) for e in evaluations],
            'clarity': [e.get('clarity_score', 0) for e in evaluations],
            'composite': [e.get('composite_score', 0) for e in evaluations]
        }

        statistics = {
            'avg_accuracy': np.mean(scores['accuracy']),
            'avg_completeness': np.mean(scores['completeness']),
            'avg_relevance': np.mean(scores['relevance']),
            'avg_clarity': np.mean(scores['clarity']),
            'avg_composite': np.mean(scores['composite']),
            'median_composite': np.median(scores['composite']),
            'std_composite': np.std(scores['composite']),
            'min_composite': np.min(scores['composite']),
            'max_composite': np.max(scores['composite'])
        }

        print(f"\nâœ“ Average Composite Score: {statistics['avg_composite']:.2f}/10")

        return {
            'evaluations': evaluations,
            'statistics': statistics,
            'total_evaluated': total
        }

    def identify_weak_areas(
        self,
        evaluations: List[Dict[str, Any]],
        threshold: float = 6.0
    ) -> Dict[str, Any]:
        """
        Identify areas where the system performs poorly.

        Args:
            evaluations: List of evaluation results
            threshold: Score threshold below which is considered weak

        Returns:
            Dictionary with weak areas and recommendations
        """
        weak_answers = [e for e in evaluations if e.get('composite_score', 0) < threshold]

        # Analyze common weaknesses
        all_weaknesses = []
        all_missing = []

        for e in weak_answers:
            all_weaknesses.extend(e.get('weaknesses', []))
            all_missing.extend(e.get('missing_information', []))

        # Count frequency of issues
        from collections import Counter
        weakness_freq = Counter(all_weaknesses)
        missing_freq = Counter(all_missing)

        return {
            'total_weak_answers': len(weak_answers),
            'percentage_weak': len(weak_answers) / len(evaluations) * 100 if evaluations else 0,
            'common_weaknesses': weakness_freq.most_common(10),
            'common_missing_info': missing_freq.most_common(10),
            'weak_answer_examples': weak_answers[:5]  # First 5 examples
        }


if __name__ == "__main__":
    # Example usage
    import os

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("Error: OPENAI_API_KEY not set")
        exit(1)

    evaluator = AccuracyEvaluator(api_key=api_key)

    # Example evaluation
    result = evaluator.evaluate_single_answer(
        question="What is cellular manufacturing?",
        generated_answer="Cellular manufacturing is a production approach that groups machines and workers into cells.",
        expected_answer="Cellular manufacturing is a lean manufacturing method where equipment and workstations are arranged in a sequence that supports a smooth flow of materials and components."
    )

    print(json.dumps(result, indent=2))
